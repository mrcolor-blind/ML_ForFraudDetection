# -*- coding: utf-8 -*-
"""Pruebasdeoutliers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WP0UFGIyrjQsv78TQN_vitNKVQBiaguP

# **Limpieza de datos (Base)**
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
base = pd.read_csv("Base.csv")
base.head()

base.shape

print(base.dtypes)

# Número total de duplicados (filas idénticas en todas las columnas)
print("Duplicados:", base.duplicated().sum())

base.info()

cat_cols = [
    "fraud_bool",
    "payment_type",
    "employment_status",
    "email_is_free",
    "housing_status",
    "phone_home_valid",
    "phone_mobile_valid",
    "has_other_cards",
    "foreign_request",
    "source",
    "device_os",
    "keep_alive_session",
    "device_fraud_count"
]
num_cols = [
    "income",
    "name_email_similarity",
    "prev_address_months_count",
    "current_address_months_count",
    "customer_age",
    "days_since_request",
    "intended_balcon_amount",
    "zip_count_4w",
    "velocity_6h",
    "velocity_24h",
    "velocity_4w",
    "bank_branch_count_8w",
    "date_of_birth_distinct_emails_4w",
    "credit_risk_score",
    "bank_months_count",
    "proposed_credit_limit",
    "session_length_in_minutes",
    "device_distinct_emails_8w",
    "month"
]

"""## Conversión de valores nulos
**Detectar:** Revisar qué columnas tienen valores nulos y cuántos.
**credit_risk_score:** es la unica variable con valores negativos válidos, segun la documentación.

***Rangos oficiales de las variables numéricas*** (*segun la documentación)*
* income: [0.1, 0.9]
* name_email_similarity: [0, 1]
* prev_address_months_count: [−1, 380] (−1 = missing)
* current_address_months_count: [−1, 429] (−1 = missing)
* customer_age: [10, 90]
* days_since_request: [0, 79]
* intended_balcon_amount: [−16, 114] (valores negativos = missing)
* zip_count_4w: [1, 6830]
* velocity_6h: [−175, 16818] (negativos = missing)
* velocity_24h: [1297, 9586]
* velocity_4w: [2825, 7020]
* bank_branch_count_8w: [0, 2404]
* date_of_birth_distinct_emails_4w: [0, 39]
* credit_risk_score: [−191, 389] (negativos son válidos aquí)
* bank_months_count: [−1, 32] (−1 = missing)
* proposed_credit_limit: [200, 2000]
* session_length_in_minutes: [−1, 107] (−1 = missing)
* device_distinct_emails_8w: [−1, 2] (−1 = missing)
* month: [0, 7]
"""

(base == -1).sum()

import numpy as np

# rangos válidos por variable numérica
ranges = {
    "income": (0.1, 0.9),
    "name_email_similarity": (0, 1),
    "prev_address_months_count": (0, 380),   # -1 = missing
    "current_address_months_count": (0, 429), # -1 = missing
    "customer_age": (10, 90),
    "days_since_request": (0, 79),
    "intended_balcon_amount": (0, 114),    # <0 = missing
    "zip_count_4w": (1, 6830),
    "velocity_6h": (0, 16818),            # <0 = missing
    "velocity_24h": (1297, 9586),
    "velocity_4w": (2825, 7020),
    "bank_branch_count_8w": (0, 2404),
    "date_of_birth_distinct_emails_4w": (0, 39),
    "credit_risk_score": (0, 389),        # negativos válidos
    "bank_months_count": (0, 32),           # -1 = missing
    "proposed_credit_limit": (200, 2000),
    "session_length_in_minutes": (0, 107),  # -1 = missing
    "device_distinct_emails_8w": (0, 2),    # -1 = missing
    "month": (0, 7)
}

# Convertir valores fuera de rango en NaN
for col, (low, high) in ranges.items():
    if col in base.columns:
        base.loc[(base[col] < low) | (base[col] > high), col] = np.nan

print("Valores fuera de rango reemplazados por NaN")

# Contar negativos por columna
negativos = (base[num_cols] < 0).sum()

print(negativos)

"""## Detección y corrección de outliers
**Detectar:**

**Métodos:** IQR (percentiles 25–75) o visualización (boxplots).

Las gráficas muestran la distribución de cada variable numérica y el boxplot. En ambas gráficas se marcan los outliers para identificar qué variables tienen outliers.

"""

import pandas as pd
import numpy as np
from pandas.api.types import is_numeric_dtype

# 1. Filtrar solo no-fraude
base_no_fraud = base[base['fraud_bool'] == 0].copy()

# 2. Detectar columnas numéricas directamente en base_no_fraud
num_cols_nf = [c for c in base_no_fraud.columns if is_numeric_dtype(base_no_fraud[c])]

# 3. Quitar binarias (0/1) y target de esa lista
cols_to_check = []
for c in num_cols_nf:
    vals = base_no_fraud[c].dropna().unique()
    if not set(vals).issubset({0, 1}):  # descartar binarias
        cols_to_check.append(c)

print("Columnas NUMÉRICAS consideradas (no binarias, solo no-fraude):")
print(cols_to_check)

# Crear una gráfica por variable

def outliers_detection(df, numeric_vars):
    cols_with_outliers = []

    for col in numeric_vars:
        if col not in df.columns:
            continue

        data = df[col].dropna()

        # Calcular IQR
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Detectar outliers
        outliers = data[(data < lower_bound) | (data > upper_bound)]

        # Guardar columna si tiene outliers
        if len(outliers) > 0:
            cols_with_outliers.append(col)

        # Crear figura
        plt.figure(figsize=(12, 5))

        # Histograma con outliers resaltados
        plt.subplot(1, 2, 1)
        sns.histplot(data, kde=True, bins=30, color="skyblue")
        plt.scatter(outliers, [0]*len(outliers), color="red", label="Outliers", zorder=5)
        plt.title(f"Distribución de {col}")
        plt.legend()

        # Boxplot
        plt.subplot(1, 2, 2)
        sns.boxplot(x=data, color="lightgreen")
        plt.title(f"Boxplot de {col}")

        plt.tight_layout()
        plt.show()

    return cols_with_outliers

cols_outliers = outliers_detection(base_no_fraud, cols_to_check)
print("Columnas con outliers de no fraude:", cols_outliers)

"""## Transformaciones para reducir outliers


"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PowerTransformer

sns.set(style="whitegrid")

def comparar_transformacion(nombre, original, transformada, metodo):
    fig, axes = plt.subplots(1, 2, figsize=(12,4))

    # original
    sns.histplot(np.ravel(original), kde=True, bins=30, color="skyblue", ax=axes[0])
    axes[0].set_title(f"{nombre} (original)")

    # transformada
    sns.histplot(np.ravel(transformada), kde=True, bins=30, color="lightgreen", ax=axes[1])
    axes[1].set_title(f"{nombre} ({metodo})")

    plt.tight_layout()
    plt.show()
    return

def log_1p(nombre, val):
    v = np.ravel(val).astype(float)
    t = np.log1p(v)
    comparar_transformacion(nombre, v, t, "log1p")
    return t

def log10_transform(nombre, val):
    v = np.ravel(val).astype(float)
    # Aquí usaremos un pequeño epsilon para no generar -inf
    v_safe = np.where(v <= 0, np.nan, v)  # Opción: poner NaN si hay 0 o negativos
    t = np.log10(v_safe)

    comparar_transformacion(nombre, v, t, "Log10")
    return t

def sqrt_transform(nombre, val):
    v = np.ravel(val).astype(float)
    v_safe = np.where(v < 0, np.nan, v)  # sqrt no acepta negativos
    t = np.sqrt(v_safe)
    comparar_transformacion(nombre, v, t, "Sqrt")
    return t

def yeo_johnson(nombre, val):
    pt = PowerTransformer(method="yeo-johnson", standardize=False)
    # Ajustar con datos de no-fraude para no distorsionar con outliers de fraude
    pt.fit(base_no_fraud[nombre].dropna().values.reshape(-1, 1).astype(float))
    t = pt.transform(val.astype(float)).ravel()
    comparar_transformacion(nombre, val, t, "Yeo-Johnson")
    return t

# prev_address_months_count ------> yeo-johnson
col = "prev_address_months_count"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# current_address_months_count ------> yeo-johnson
col = "current_address_months_count"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# days_since_request -------> yeo-johnson
col = "days_since_request"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# intended_balcon_amount ------> yeo-johnson
col = "intended_balcon_amount"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# zip_count_4w -----> yeo-johnson
col = "zip_count_4w"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_6h -----> yeo-johnson
col = "velocity_6h"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_24h -----> yeo-johnson
col = "velocity_24h"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# bank_branch_count_8w ------> yeo-johnson
col = "bank_branch_count_8w"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# date_of_birth_distinct_emails_4w ------> log1p
col = "date_of_birth_distinct_emails_4w"
m = base[col].notna()
base.loc[m, col] = log_1p(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# credit_risk_score -------> yeo-johnson
col = "credit_risk_score"
m = base[col].notna()
base.loc[m, col] = yeo_johnson(col, base.loc[m, col].to_numpy().reshape(-1, 1))

# session_length_in_minutes ------> log1p
col = "session_length_in_minutes"
m = base[col].notna()
base.loc[m, col] = log_1p(col, base.loc[m, col].to_numpy().reshape(-1, 1))

"""## Imputar valores NaN"""

import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype

# Columnas donde 0 = ausencia real (según BAF/Base.csv)
absence_real = [c for c in ["prev_address_months_count",
                            "bank_months_count",
                            "intended_balcon_amount"]
                if c in base.columns]

# Crear indicadores *_was_missing solo en numéricas con NaN
cols_with_nan = [c for c in num_cols if base[c].isna().any()]
for c in cols_with_nan:
    base[f"{c}_was_missing"] = base[c].isna().astype("int8")

# Imputación:
# 0 para ausencia real
for c in absence_real:
    base[c] = base[c].fillna(0)

# Mediana para el resto de numéricas con NaN
median_cols = [c for c in cols_with_nan if c not in absence_real]
for c in median_cols:
    base[c] = base[c].fillna(base[c].median())

"""## Codificación de variables categoricas"""

for col in cat_cols:
    print(base[col].value_counts())
    print('-'*20)

"""### ONE-HOT ENCODING para categóricas"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_numeric_dtype

cat_cols = ["payment_type","employment_status","housing_status","source","device_os"]
enc = OneHotEncoder(handle_unknown="ignore", dtype=np.uint8)
one_hot = enc.fit_transform(base[cat_cols])

one_hot_df = pd.DataFrame(
    one_hot.toarray(),  # Convert sparse matrix to dense array
    columns=enc.get_feature_names_out(cat_cols),
    index=base.index
)

base_clean = pd.concat([base.drop(columns=cat_cols), one_hot_df], axis=1)

print("Dataset limpio listo:", base_clean.shape)

"""## Descargar"""

# Exportar a CSV
base_clean.to_csv("base_clean.csv", index=False)

from google.colab import drive
drive.mount('/content/drive')

"""# **Limpieza de datos (Variante II)**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
vii = pd.read_csv("Variant II.csv")
vii.head()

vii.shape

print(vii.dtypes)

# Número total de duplicados (filas idénticas en todas las columnas)
print("Duplicados:", vii.duplicated().sum())

vii.info()

"""## Conversión de valores nulos"""

(vii == -1).sum()

# Convertir valores fuera de rango en NaN
for col, (low, high) in ranges.items():
    if col in vii.columns:
        vii.loc[(vii[col] < low) | (vii[col] > high), col] = np.nan

print("Valores fuera de rango reemplazados por NaN")

# Contar negativos por columna
negativos = (vii[num_cols] < 0).sum()

print(negativos)

"""## Detección y corrección de outliers"""

import pandas as pd
import numpy as np
from pandas.api.types import is_numeric_dtype

# 1. Filtrar solo no-fraude
vii_no_fraud = vii[vii['fraud_bool'] == 0].copy()

# 2. Detectar columnas numéricas directamente en vii_no_fraud
num_cols_nf = [c for c in vii_no_fraud.columns if is_numeric_dtype(vii_no_fraud[c])]

# 3. Quitar binarias (0/1) y target de esa lista
cols_to_check2 = []
for c in num_cols_nf:
    vals = vii_no_fraud[c].dropna().unique()
    if not set(vals).issubset({0, 1}):  # descartar binarias
        cols_to_check2.append(c)

print("Columnas NUMÉRICAS consideradas (no binarias, solo no-fraude):")
print(cols_to_check2)

cols_outliers = outliers_detection(vii_no_fraud, cols_to_check2)
print("Columnas con outliers de no fraude:", cols_outliers)

"""---



*   prev_address_months_count
* current_address_months_count
* days_since_request
* intended_balcon_amount
* zip_count_4w
* velocity_6h
* velocity_24h
* velocity_4w
* bank_branch_count_8w
* date_of_birth_distinct_emails_4w
* credit_risk_score
* session_length_in_minutes

## Transformaciones
"""

def yeo_johnson(nombre, val):
    pt = PowerTransformer(method="yeo-johnson", standardize=False)
    # Ajustar con datos de no-fraude para no distorsionar con outliers de fraude
    pt.fit(vii_no_fraud[nombre].dropna().values.reshape(-1, 1).astype(float))
    t = pt.transform(val.astype(float)).ravel()
    comparar_transformacion(nombre, val, t, "Yeo-Johnson")
    return t

# prev_address_months_count ------> log1p
col = "prev_address_months_count"
m = vii[col].notna()  # tras la limpieza
vii.loc[m, col] = log_1p(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# current_address_months_count ------> yeo-johnson
col = "current_address_months_count"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# days_since_request -------> log10
col = "days_since_request"
m = vii[col].notna()
vii.loc[m, col] = log10_transform(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# intended_balcon_amount ------> yeo-johnson
col = "intended_balcon_amount"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# zip_count_4w -----> yeo-johnson
col = "zip_count_4w"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_6h -----> yeo-johnson
col = "velocity_6h"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_24h -----> yeo-johnson
col = "velocity_24h"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# bank_branch_count_8w ------> yeo-johnson
col = "bank_branch_count_8w"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# date_of_birth_distinct_emails_4w ------> yeo-johnson
col = "date_of_birth_distinct_emails_4w"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# credit_risk_score -------> yeo-johnson
col = "credit_risk_score"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

# session_length_in_minutes ------> yeo-johnson
col = "session_length_in_minutes"
m = vii[col].notna()
vii.loc[m, col] = yeo_johnson(col, vii.loc[m, col].to_numpy().reshape(-1, 1))

"""## Imputar valores NaN"""

import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype

# Columnas donde 0 = ausencia real (según BAF/vii.csv)
absence_real = [c for c in ["prev_address_months_count",
                            "bank_months_count",
                            "intended_balcon_amount"]
                if c in vii.columns]

# Crear indicadores *_was_missing solo en numéricas con NaN
cols_with_nan = [c for c in num_cols if vii[c].isna().any()]
for c in cols_with_nan:
    vii[f"{c}_was_missing"] = vii[c].isna().astype("int8")

# Imputación:
# 0 para ausencia real
for c in absence_real:
    vii[c] = vii[c].fillna(0)

# Mediana para el resto de numéricas con NaN
median_cols = [c for c in cols_with_nan if c not in absence_real]
for c in median_cols:
    vii[c] = vii[c].fillna(vii[c].median())

"""## Codificación de variables categoricas"""

for col in cat_cols:
    print(vii[col].value_counts())
    print('-'*20)

"""### ONE-HOT ENCODING para categóricas"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_numeric_dtype

cat_cols = ["payment_type","employment_status","housing_status","source","device_os"]
enc = OneHotEncoder(handle_unknown="ignore", dtype=np.uint8)
one_hot = enc.fit_transform(vii[cat_cols])

one_hot_df = pd.DataFrame(
    one_hot.toarray(),  # Convert sparse matrix to dense array
    columns=enc.get_feature_names_out(cat_cols),
    index=vii.index
)

vii_clean = pd.concat([vii.drop(columns=cat_cols), one_hot_df], axis=1)

print("Dataset limpio listo:", vii_clean.shape)

"""## Descargar"""

# Exportar a CSV
vii_clean.to_csv("vii_clean.csv", index=False)

"""# **Limpieza de datos (Variante III)**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
viii = pd.read_csv("Variant III.csv")
viii.head()

viii.shape

print(viii.dtypes)

# Número total de duplicados (filas idénticas en todas las columnas)
print("Duplicados:", viii.duplicated().sum())

viii.info()

"""## Conversión de valores nulos"""

(viii == -1).sum()

# Convertir valores fuera de rango en NaN
for col, (low, high) in ranges.items():
    if col in viii.columns:
        viii.loc[(viii[col] < low) | (viii[col] > high), col] = np.nan

print("Valores fuera de rango reemplazados por NaN")

# Contar negativos por columna
negativos = (viii[num_cols] < 0).sum()

print(negativos)

"""## Detección y corrección de outliers"""

import pandas as pd
import numpy as np
from pandas.api.types import is_numeric_dtype

# 1. Filtrar solo no-fraude
viii_no_fraud = viii[viii['fraud_bool'] == 0].copy()

# 2. Detectar columnas numéricas directamente en viii_no_fraud
num_cols_nf = [c for c in viii_no_fraud.columns if is_numeric_dtype(viii_no_fraud[c])]

# 3. Quitar binarias (0/1) y target de esa lista
cols_to_check3 = []
for c in num_cols_nf:
    vals = viii_no_fraud[c].dropna().unique()
    if not set(vals).issubset({0, 1}):  # descartar binarias
        cols_to_check3.append(c)

print("Columnas NUMÉRICAS consideradas (no binarias, solo no-fraude):")
print(cols_to_check3)

cols_outliers = outliers_detection(viii_no_fraud, cols_to_check3)
print("Columnas con outliers de no fraude:", cols_outliers)

"""## Transformaciones
* prev_address_months_count
* current_address_months_count
* customer_age
* days_since_request
* intended_balcon_amount
* zip_count_4w
* velocity_6h
* velocity_24h
* bank_branch_count_8w
* date_of_birth_distinct_emails_4w
* credit_risk_score
* session_length_in_minutes
* device_distinct_emails_8w
"""

def yeo_johnson(nombre, val):
    pt = PowerTransformer(method="yeo-johnson", standardize=False)
    # Ajustar con datos de no-fraude para no distorsionar con outliers de fraude
    pt.fit(viii_no_fraud[nombre].dropna().values.reshape(-1, 1).astype(float))
    t = pt.transform(val.astype(float)).ravel()
    comparar_transformacion(nombre, val, t, "Yeo-Johnson")
    return t
# prev_address_months_count ------> log10
col = "prev_address_months_count"
m = viii[col].notna()
viii.loc[m, col] = log10_transform(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# current_address_months_count ------> yeo-johnson
col = "current_address_months_count"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# days_since_request -------> yeo-johnson
col = "days_since_request"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# intended_balcon_amount ------> yeo-johnson
col = "intended_balcon_amount"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# zip_count_4w -----> log10
col = "zip_count_4w"
m = viii[col].notna()
viii.loc[m, col] = log10_transform(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_6h -----> sqrt
col = "velocity_6h"
m = viii[col].notna()
viii.loc[m, col] = sqrt_transform(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_24h -----> yeo-johnson
col = "velocity_24h"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# bank_branch_count_8w ------> yeo-johnson
col = "bank_branch_count_8w"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# date_of_birth_distinct_emails_4w ------> sqrt
col = "date_of_birth_distinct_emails_4w"
m = viii[col].notna()
viii.loc[m, col] = sqrt_transform(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# credit_risk_score -------> yeo-johnson
col = "credit_risk_score"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

# session_length_in_minutes ------> yeo-johnson
col = "session_length_in_minutes"
m = viii[col].notna()
viii.loc[m, col] = yeo_johnson(col, viii.loc[m, col].to_numpy().reshape(-1, 1))

"""## Imputar valores NaN"""

import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype

# Columnas donde 0 = ausencia real (según BAF/Viii.csv)
absence_real = [c for c in ["prev_address_months_count",
                            "bank_months_count",
                            "intended_balcon_amount"]
                if c in viii.columns]

# Crear indicadores *_was_missing solo en numéricas con NaN
cols_with_nan = [c for c in num_cols if viii[c].isna().any()]
for c in cols_with_nan:
    viii[f"{c}_was_missing"] = viii[c].isna().astype("int8")

# Imputación:
# 0 para ausencia real
for c in absence_real:
    viii[c] = viii[c].fillna(0)

# Mediana para el resto de numéricas con NaN
median_cols = [c for c in cols_with_nan if c not in absence_real]
for c in median_cols:
    viii[c] = viii[c].fillna(viii[c].median())

"""## Codificación de variables categoricas"""

for col in cat_cols:
    print(viii[col].value_counts())
    print('-'*20)

"""### ONE-HOT ENCODING para categóricas"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_numeric_dtype

cat_cols = ["payment_type","employment_status","housing_status","source","device_os"]
enc = OneHotEncoder(handle_unknown="ignore", dtype=np.uint8)
one_hot = enc.fit_transform(viii[cat_cols])

one_hot_df = pd.DataFrame(
    one_hot.toarray(),  # Convert sparse matrix to dense array
    columns=enc.get_feature_names_out(cat_cols),
    index=viii.index
)

viii_clean = pd.concat([viii.drop(columns=cat_cols), one_hot_df], axis=1)

print("Dataset limpio listo:", viii_clean.shape)

"""## Descargar"""

# Exportar a CSV
viii_clean.to_csv("viii_clean.csv", index=False)

"""# **Limpieza de datos (Variante V)**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
vv = pd.read_csv("Variant V.csv")
vv.head()

vv.shape

print(vv.dtypes)

# Número total de duplicados (filas idénticas en todas las columnas)
print("Duplicados:", vv.duplicated().sum())

vv.info()

"""## Conversión de valores nulos"""

(vv == -1).sum()

# Convertir valores fuera de rango en NaN
for col, (low, high) in ranges.items():
    if col in vv.columns:
        vv.loc[(vv[col] < low) | (vv[col] > high), col] = np.nan

print("Valores fuera de rango reemplazados por NaN")

# Contar negativos por columna
negativos = (vv[num_cols] < 0).sum()

print(negativos)

"""## Detección y corrección de outliers"""

import pandas as pd
import numpy as np
from pandas.api.types import is_numeric_dtype

# 1. Filtrar solo no-fraude
vv_no_fraud = vv[vv['fraud_bool'] == 0].copy()

# 2. Detectar columnas numéricas directamente en vv_no_fraud
num_cols_nf = [c for c in vv_no_fraud.columns if is_numeric_dtype(vv_no_fraud[c])]

# 3. Quitar binarias (0/1) y target de esa lista
cols_to_check5 = []
for c in num_cols_nf:
    vals = vv_no_fraud[c].dropna().unique()
    if not set(vals).issubset({0, 1}):  # descartar binarias
        cols_to_check5.append(c)

print("Columnas NUMÉRICAS consideradas (no binarias, solo no-fraude):")
print(cols_to_check5)

cols_outliers = outliers_detection(vv_no_fraud, cols_to_check5)
print("Columnas con outliers de no fraude:", cols_outliers)

"""## Transformaciones

* prev\_address\_months\_count
* current\_address\_months\_count
* days\_since\_request
* intended\_balcon\_amount
* zip\_count\_4w
* velocity\_6h
* velocity\_24h
* velocity\_4w
* bank\_branch\_count\_8w
* date\_of\_birth\_distinct\_emails\_4w
* credit\_risk\_score
* session\_length\_in\_minutes
* device\_distinct\_emails\_8w
* x1
* x2
"""

def yeo_johnson(nombre, val):
    pt = PowerTransformer(method="yeo-johnson", standardize=False)
    # Ajustar con datos de no-fraude para no distorsionar con outliers de fraude
    pt.fit(vv_no_fraud[nombre].dropna().values.reshape(-1, 1).astype(float))
    t = pt.transform(val.astype(float)).ravel()
    comparar_transformacion(nombre, val, t, "Yeo-Johnson")
    return t
# prev_address_months_count ------> yeo-johnson
col = "prev_address_months_count"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# current_address_months_count ------> yeo-johnson
col = "current_address_months_count"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# days_since_request -------> yeo-johnson
col = "days_since_request"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# intended_balcon_amount ------> yeo-johnson
col = "intended_balcon_amount"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# zip_count_4w -----> log1p
col = "zip_count_4w"
m = vv[col].notna()
vv.loc[m, col] = log_1p(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_6h -----> log1p
col = "velocity_6h"
m = vv[col].notna()
vv.loc[m, col] = log_1p(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# velocity_24h -----> yeo-johnson
col = "velocity_24h"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# bank_branch_count_8w ------> log1p
col = "bank_branch_count_8w"
m = vv[col].notna()
vv.loc[m, col] = log_1p(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# date_of_birth_distinct_emails_4w ------> yeo-johnson
col = "date_of_birth_distinct_emails_4w"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# credit_risk_score -------> yeo-johnson
col = "credit_risk_score"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

# session_length_in_minutes ------> yeo-johnson
col = "session_length_in_minutes"
m = vv[col].notna()
vv.loc[m, col] = yeo_johnson(col, vv.loc[m, col].to_numpy().reshape(-1, 1))

"""## Imputar valores NaN"""

import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype

# Columnas donde 0 = ausencia real (según BAF/vv.csv)
absence_real = [c for c in ["prev_address_months_count",
                            "bank_months_count",
                            "intended_balcon_amount"]
                if c in vv.columns]

# Crear indicadores *_was_missing solo en numéricas con NaN
cols_with_nan = [c for c in num_cols if vv[c].isna().any()]
for c in cols_with_nan:
    vv[f"{c}_was_missing"] = vv[c].isna().astype("int8")

# Imputación:
# 0 para ausencia real
for c in absence_real:
    vv[c] = vv[c].fillna(0)

# Mediana para el resto de numéricas con NaN
median_cols = [c for c in cols_with_nan if c not in absence_real]
for c in median_cols:
    vv[c] = vv[c].fillna(vv[c].median())

"""## Codificacion de variables categoricas"""

for col in cat_cols:
    print(vv[col].value_counts())
    print('-'*20)

"""### ONE HOT ENCODING para categoricas"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_numeric_dtype

cat_cols = ["payment_type","employment_status","housing_status","source","device_os"]
enc = OneHotEncoder(handle_unknown="ignore", dtype=np.uint8)
one_hot = enc.fit_transform(vv[cat_cols])

one_hot_df = pd.DataFrame(
    one_hot.toarray(),  # Convert sparse matrix to dense array
    columns=enc.get_feature_names_out(cat_cols),
    index=vv.index
)

vv_clean = pd.concat([vv.drop(columns=cat_cols), one_hot_df], axis=1)

print("Dataset limpio listo:", vv_clean.shape)

"""## Descargar"""

# Exportar a CSV
vv_clean.to_csv("vv_clean.csv", index=False)

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
base_clean = pd.read_csv("base_clean.csv")
base_clean.head()
base_fraud = base_clean[base_clean['fraud_bool'] == 1].copy()
base_fraud

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
vii_clean = pd.read_csv("vii_clean.csv")
vii_clean.head()
vii_fraud = vii_clean[vii_clean['fraud_bool'] == 1].copy()
vii_fraud

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
viii_clean = pd.read_csv("viii_clean.csv")
viii_clean.head()
viii_fraud = viii_clean[viii_clean['fraud_bool'] == 1].copy()
viii_fraud

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
vv_clean = pd.read_csv("vv_clean.csv")
vv_clean.head()
vv_fraud = vv_clean[vv_clean['fraud_bool'] == 1].copy()
vv_fraud